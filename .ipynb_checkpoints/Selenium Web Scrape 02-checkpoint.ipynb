{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4b8674-3a64-4411-8482-f28310839f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCRAPE MULTIPLE FINANCIAL PARAMETERS FROM SINGLE PAGE / V2 / Faster scrolling using drag&drop apprach\n",
    "#https://stackoverflow.com/questions/62119348/how-to-scroll-horizontally-using-selenium-chromedriver-in-python\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260d96d4-8724-483f-abc2-6ad3457a4103",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ToDo\n",
    "#Add Support for balanse-sheet,cashflow-statement,financial-ratios and other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "252d804e-f8b5-41a3-ba8d-d034022a6303",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "53 columns passed, passed data had 56 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:982\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 982\u001b[0m     columns \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_or_indexify_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:1030\u001b[0m, in \u001b[0;36m_validate_or_indexify_columns\u001b[1;34m(content, columns)\u001b[0m\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_mi_list \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(content):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m   1029\u001b[0m     \u001b[38;5;66;03m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[1;32m-> 1030\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m   1031\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns passed, passed data had \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1032\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(content)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1033\u001b[0m     )\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_mi_list:\n\u001b[0;32m   1035\u001b[0m \n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;66;03m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: 53 columns passed, passed data had 56 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [328]\u001b[0m, in \u001b[0;36m<cell line: 138>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_scraped_data\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# apple = ScrapeMacrotrend(\"https://www.macrotrends.net/stocks/charts/AAPL/apple/income-statement?freq=Q\")\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# apple = ScrapeMacrotrend(\"aapl\")\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m tesla \u001b[38;5;241m=\u001b[39m \u001b[43mScrapeMacrotrend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtesla\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [328]\u001b[0m, in \u001b[0;36mScrapeMacrotrend.__init__\u001b[1;34m(self, ticker_name)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompany_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetCompanyURL()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#scrape main pages\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# self.income_statement = self.runScraper(self.company_url + \"/income-statement?freq=Q\")\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbalance_sheet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunScraper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompany_url\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/balance-sheet?freq=Q\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# self.cash_flow_statement = self.runScraper(self.company_url + \"/cash-flow-statement?freq=Q\")\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# self.financial_ratios = self.runScraper(self.company_url + \"/financial-ratios?freq=Q\")\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver\u001b[38;5;241m.\u001b[39mclose() \u001b[38;5;66;03m#close chrome-webdriver\u001b[39;00m\n",
      "Input \u001b[1;32mIn [328]\u001b[0m, in \u001b[0;36mScrapeMacrotrend.runScraper\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m#add the scraped data to pandas dataframe\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m (\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmain_table_data)):\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;66;03m#create temp dataframe with scraped data\u001b[39;00m\n\u001b[1;32m--> 127\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain_table_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperiods_table_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain_table_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m    129\u001b[0m     \u001b[38;5;66;03m#append temp-dataframe row to global dataframe\u001b[39;00m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_scraped_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_scraped_data, data]) \n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:721\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    716\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    717\u001b[0m         \u001b[38;5;66;03m# error: Argument 1 to \"ensure_index\" has incompatible type\u001b[39;00m\n\u001b[0;32m    718\u001b[0m         \u001b[38;5;66;03m# \"Collection[Any]\"; expected \"Union[Union[Union[ExtensionArray,\u001b[39;00m\n\u001b[0;32m    719\u001b[0m         \u001b[38;5;66;03m# ndarray], Index, Series], Sequence[Any]]\"\u001b[39;00m\n\u001b[0;32m    720\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m \u001b[43mnested_data_to_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[0;32m    723\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[0;32m    724\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    725\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    727\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    728\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    729\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[0;32m    730\u001b[0m         arrays,\n\u001b[0;32m    731\u001b[0m         columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    734\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    735\u001b[0m     )\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:519\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[1;34m(data, columns, index, dtype)\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    517\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[1;32m--> 519\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m \u001b[43mto_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    520\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:883\u001b[0m, in \u001b[0;36mto_arrays\u001b[1;34m(data, columns, dtype)\u001b[0m\n\u001b[0;32m    880\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m    881\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[1;32m--> 883\u001b[0m content, columns \u001b[38;5;241m=\u001b[39m \u001b[43m_finalize_columns_and_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:985\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    982\u001b[0m     columns \u001b[38;5;241m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n\u001b[1;32m--> 985\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(contents) \u001b[38;5;129;01mand\u001b[39;00m contents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n\u001b[0;32m    988\u001b[0m     contents \u001b[38;5;241m=\u001b[39m _convert_object_array(contents, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mValueError\u001b[0m: 53 columns passed, passed data had 56 columns"
     ]
    }
   ],
   "source": [
    "class ScrapeMacrotrend:\n",
    "    \n",
    "    def __init__(self, ticker_name):\n",
    "        \n",
    "        self.start = time.time() #start timer\n",
    "        self.ticker_name = ticker_name #ticker or company name\n",
    "        \n",
    "        #innitialize and set chrome-webdriver options\n",
    "        self.chrome_options = Options()\n",
    "        #self.chrome_options.add_argument(\"--window-size=1000,1080\")\n",
    "        #self.chrome_options.add_argument(\"--headless\")\n",
    "        \n",
    "        self.driver = webdriver.Chrome(\"G:\\My Drive\\Investing\\Programming\\chromedriver.exe\", options=self.chrome_options)\n",
    "        self.driver.implicitly_wait(5)\n",
    "        self.driver.maximize_window()\n",
    "        self.base_url = \"https://www.macrotrends.net/\"\n",
    "        \n",
    "        self.ad_clicked = False\n",
    "        self.company_url = self.getCompanyURL()\n",
    "        \n",
    "        #scrape main pages\n",
    "        # self.income_statement = self.runScraper(self.company_url + \"/income-statement?freq=Q\")\n",
    "        self.balance_sheet = self.runScraper(self.company_url + \"/balance-sheet?freq=Q\")\n",
    "        # self.cash_flow_statement = self.runScraper(self.company_url + \"/cash-flow-statement?freq=Q\")\n",
    "        # self.financial_ratios = self.runScraper(self.company_url + \"/financial-ratios?freq=Q\")\n",
    "        \n",
    "        self.driver.close() #close chrome-webdriver\n",
    "        self.end = time.time() #stop timer\n",
    "        print('Complete time: {}s'.format(self.end - self.start))\n",
    "        \n",
    "        \n",
    "    def getCompanyURL(self): #this function finds the company url f.e. https://www.macrotrends.net/stocks/charts/AAPL/apple\n",
    "        self.driver.get(self.base_url)\n",
    "        self.search_box = self.driver.find_element_by_css_selector(\"input.js-typeahead\")\n",
    "        self.search_box.send_keys(self.ticker_name)\n",
    "        \n",
    "        #get company url\n",
    "        self.company_url = self.driver.find_element_by_xpath(\"//li[1]/a\") #xpath element of the company urls\n",
    "        self.company_url = self.company_url.get_attribute('href') #get the href-value\n",
    "        self.company_url = self.company_url.rsplit(\"/\", 1)[0] #split url to remove unnecesery data\n",
    "        \n",
    "        return self.company_url\n",
    "        \n",
    "        \n",
    "    def runScraper(self, url):\n",
    "        \n",
    "        #scraper variables\n",
    "        self.periods_table_prev_last_elem = -1 #previously(before scrolling) last scraped element of periods-table\n",
    "        self.periods_table_current_last_elem = 0 #current(after scrolling) last scraped element of periods-table\n",
    "        self.main_table_prev_last_elem = -1  #previously(before scrolling) last scraped element of main-table\n",
    "        self.main_table_current_last_elem = 0 #current(after scrolling) last scraped element of main-table\n",
    "\n",
    "        #data-storring variables\n",
    "        self.periods_table_data = [] #store data from periods table (annual or quarter)\n",
    "        self.main_table_data = [] #store data from the main-table like revenue, \n",
    "        self.all_scraped_data = pd.DataFrame()\n",
    "        \n",
    "        self.driver.get(url)\n",
    "\n",
    "        #find advertise window button and click it\n",
    "        if self.ad_clicked == False:\n",
    "            self.ad_button = self.driver.find_element_by_xpath('//button[@class=\"Button__StyledButton-a1qza5-0 cUAUIG\"]')\n",
    "            self.ad_button.click()\n",
    "            self.ad_clicked = True\n",
    "\n",
    "\n",
    "        self.horizontal_bar_width = self.driver.find_element_by_id('jqxScrollOuterWraphorizontalScrollBarjqxgrid').rect['width']\n",
    "        self.slider = self.driver.find_element_by_id('jqxScrollThumbhorizontalScrollBarjqxgrid')\n",
    "\n",
    "        #raise SystemExit(\"Stop right there!\")\n",
    "\n",
    "        #scroll table from left to right and scrape data\n",
    "        while (self.periods_table_prev_last_elem != self.periods_table_current_last_elem): #and (self.main_table_prev_last_elem != self.main_table_current_last_elem):\n",
    "            \n",
    "            #set the last element from the last scrape as previous one\n",
    "            self.periods_table_prev_last_elem = self.periods_table_current_last_elem\n",
    "            self.main_table_prev_last_elem = self.main_table_current_last_elem\n",
    "\n",
    "            # self.periods_table = self.driver.find_element_by_xpath('//*[@id=\"columntablejqxgrid\"]') \n",
    "            self.periods_table = self.driver.find_element_by_xpath('//*[@id=\"contentjqxgrid\"]/div[1]') #this is the header row in the table, containing the dates.\n",
    "\n",
    "            #parameters_table = self.driver.find_element_by_xpath('//*[@id=\"contentjqxgrid\"]/div[2]')\n",
    "            self.periods_table_split = self.periods_table.text.splitlines() #split the periods to array\n",
    "            self.periods_table_current_last_elem = self.periods_table_split[-1] #get the last element from the table\n",
    "            # print(self.periods_table.text)\n",
    "            # print('last-element={}'.format(self.periods_table_current_last_elem))\n",
    "\n",
    "            # main_table = self.driver.find_elements_by_xpath('//div[@role=\"row\"]') #get the main table\n",
    "            main_table = self.driver.find_elements_by_xpath('//*[@id=\"contentjqxgrid\"]/div[2]/div/div') #get the main table\n",
    "            self.main_table_current_last_elem = main_table[0].text.splitlines()[-1]\n",
    "\n",
    "            #this is the innitial position of the table, no scrolling has been executed yet\n",
    "            if self.periods_table_prev_last_elem == 0: \n",
    "                #add scraped elements of periods table to a temp-variable \n",
    "                self.periods_table_data.extend(self.periods_table_split) \n",
    "                \n",
    "                #add the scraped elements of main-table to a temp-variable\n",
    "                for i in range (len(main_table)): #add all parameter to table\n",
    "                    main_table_split = main_table[i].text.splitlines()\n",
    "                    self.main_table_data.append(main_table_split)\n",
    "\n",
    "            else:\n",
    "                #find the previous last element when the periods-table is scrolled to the right\n",
    "                self.periods_table_prev_last_elem_index = self.periods_table_split.index(self.periods_table_prev_last_elem)\n",
    "                #add the new elements after the periods-table is scrolled to the right\n",
    "                self.periods_table_data.extend(self.periods_table_split[self.periods_table_prev_last_elem_index+1:]) \n",
    "                \n",
    "                #find the previous last element when the main-table is scrolled to the right\n",
    "                self.main_table_prev_last_elem_index = main_table[0].text.splitlines().index(self.main_table_prev_last_elem) \n",
    "                #add the new elements after the main-table is scrolled to the right\n",
    "                for row in range (len(main_table)): #add the new data to the table after scrolling table to the right\n",
    "                    main_table_split = main_table[row].text.splitlines()\n",
    "                    self.main_table_data[row].extend(main_table_split[self.main_table_prev_last_elem_index+1:])\n",
    "\n",
    "\n",
    "                # raise SystemExit(\"Stop right there!\")\n",
    "\n",
    "            # Ensure the slider is in view\n",
    "            self.slider.location_once_scrolled_into_view\n",
    "            #scroll table to the right\n",
    "            ActionChains(self.driver).click_and_hold(self.slider).move_by_offset(self.horizontal_bar_width/5, 0).release().perform()\n",
    "\n",
    "        \n",
    "        #add the scraped data to pandas dataframe\n",
    "        for i in range (len(self.main_table_data)):\n",
    "            #create temp dataframe with scraped data\n",
    "            data = pd.DataFrame([self.main_table_data[i][1:]], columns=self.periods_table_data[1:], index=[self.main_table_data[i][0]]) \n",
    "            \n",
    "            #append temp-dataframe row to global dataframe\n",
    "            self.all_scraped_data = pd.concat([self.all_scraped_data, data]) \n",
    "\n",
    "        \n",
    "        #return scraped data\n",
    "        return self.all_scraped_data\n",
    "        \n",
    "# apple = ScrapeMacrotrend(\"https://www.macrotrends.net/stocks/charts/AAPL/apple/income-statement?freq=Q\")\n",
    "# apple = ScrapeMacrotrend(\"aapl\")\n",
    "tesla = ScrapeMacrotrend(\"tesla\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46832d9-cc4e-4c87-9c83-76bec4211338",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple.financial_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b675dc3-dadd-4bdd-be32-13e204e1c437",
   "metadata": {},
   "outputs": [],
   "source": [
    "periods_table_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7673c4-8eb5-43ad-9bd9-242e5bf34931",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677141ef-ba7c-4855-8d9f-cdd2511a836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove \"$\" from the values\n",
    "def fixData(input_data):\n",
    "    \n",
    "    for i in range(len(input_data)):\n",
    "        input_data[i]=input_data[i].replace(\"$\",\"\")\n",
    "        input_data[i]=input_data[i].replace(\",\",\".\")\n",
    "        # try:\n",
    "        #     input_data[i] =  float(input_data[i])\n",
    "        #     # print(type(input_data[i]), input_data[i])\n",
    "        # except:\n",
    "        #     pass\n",
    "        #     # print(input_data[i],\"can't convert to float\")\n",
    "        \n",
    "        try:\n",
    "            input_data[i] = pd.to_numeric(input_data[i])\n",
    "            # print(type(input_data[i]), \"converted to numeric\")\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "            # print(\"Can't convert to numeric\")\n",
    "        \n",
    "    return input_data\n",
    "    \n",
    "\n",
    "c_data = company_data.copy()\n",
    "c_data.apply(fixData)\n",
    "# c_data.apply(pd.to_numeric)\n",
    "c_data = c_data.T #transpose the table\n",
    "# c_data.index.to_datetime()\n",
    "c_data.index = pd.to_datetime(c_data.index) #change index data-type to datetime.\n",
    "c_data\n",
    "\n",
    "# c_data[\"Revenue\"] = pd.to_numeric(c_data[\"Revenue\"]) #convert data-type from string to number\n",
    "# c_data[\"Gross Profit\"] = pd.to_numeric(c_data[\"Gross Profit\"]) #convert data-type from string to number\n",
    "c_data[\"Revenue\"].plot()\n",
    "c_data[\"Gross Profit\"].plot()\n",
    "plt.show()\n",
    "\n",
    "# company_data\n",
    "#DataFrame.apply(func, axis=0, raw=False, result_type=None, args=(), **kwargs)\n",
    "# df = pd.DataFrame({\n",
    "#    'pig': [20, 18, 489, 675, 1776],\n",
    "#    'horse': [4, 25, 281, 600, 1900]\n",
    "#    }, index=[1990, 1997, 2003, 2009, 2014])\n",
    "# lines = df.plot.line()\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d95c77e-b7fb-474b-970d-f4dd1edfb9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_data.T.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
