{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4b8674-3a64-4411-8482-f28310839f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCRAPE MULTIPLE FINANCIAL PARAMETERS FROM SINGLE PAGE / V2 / Faster scrolling using drag&drop apprach\n",
    "#https://stackoverflow.com/questions/62119348/how-to-scroll-horizontally-using-selenium-chromedriver-in-python\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260d96d4-8724-483f-abc2-6ad3457a4103",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ToDo\n",
    "#Add Support for balanse-sheet,cashflow-statement,financial-ratios and other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252d804e-f8b5-41a3-ba8d-d034022a6303",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScrapeMacrotrend:\n",
    "    \n",
    "    def __init__(self, ticker_name):\n",
    "        \n",
    "        self.start = time.time() #start timer\n",
    "        self.ticker_name = ticker_name #ticker or company name\n",
    "        \n",
    "        #innitialize and set chrome-webdriver options\n",
    "        self.chrome_options = Options()\n",
    "        #self.chrome_options.add_argument(\"--window-size=1000,1080\")\n",
    "        #self.chrome_options.add_argument(\"--headless\")\n",
    "        \n",
    "        self.driver = webdriver.Chrome(\"G:\\My Drive\\Investing\\Programming\\chromedriver.exe\", options=self.chrome_options)\n",
    "        self.driver.implicitly_wait(5)\n",
    "        self.driver.maximize_window()\n",
    "        self.base_url = \"https://www.macrotrends.net/\"\n",
    "        \n",
    "        self.ad_clicked = False\n",
    "        self.company_url = self.getCompanyURL()\n",
    "        \n",
    "        #scrape main pages\n",
    "        # self.income_statement = self.runScraper(self.company_url + \"/income-statement?freq=Q\")\n",
    "        # self.balance_sheet = self.runScraper(self.company_url + \"/balance-sheet?freq=Q\")\n",
    "        # self.cash_flow_statement = self.runScraper(self.company_url + \"/cash-flow-statement?freq=Q\")\n",
    "        self.temp_data = []\n",
    "        # self.financial_ratios = self.runScraper(self.company_url + \"/financial-ratios?freq=Q\")\n",
    "        self.financial_ratios = self.runFullTableScrape(self.company_url + \"/financial-ratios?freq=Q\")\n",
    "\n",
    "        \n",
    "        self.driver.close() #close chrome-webdriver\n",
    "        self.end = time.time() #stop timer\n",
    "        print('Complete time: {}s'.format(self.end - self.start))\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    def getCompanyURL(self): #this function finds the company url f.e. https://www.macrotrends.net/stocks/charts/AAPL/apple\n",
    "        self.driver.get(self.base_url)\n",
    "        self.search_box = self.driver.find_element_by_css_selector(\"input.js-typeahead\")\n",
    "        self.search_box.send_keys(self.ticker_name)\n",
    "        \n",
    "        #get company url\n",
    "        self.company_url = self.driver.find_element_by_xpath(\"//li[1]/a\") #xpath element of the company urls\n",
    "        self.company_url = self.company_url.get_attribute('href') #get the href-value\n",
    "        self.company_url = self.company_url.rsplit(\"/\", 1)[0] #split url to remove unnecesery data\n",
    "        \n",
    "        return self.company_url\n",
    "        \n",
    "        \n",
    "    def runScraper(self, url):\n",
    "        \n",
    "        #scraper variables\n",
    "        self.periods_table_prev_last_elem = -1 #previously(before scrolling) last scraped element of periods-table\n",
    "        self.periods_table_current_last_elem = 0 #current(after scrolling) last scraped element of periods-table\n",
    "        self.main_table_prev_last_elem = -1  #previously(before scrolling) last scraped element of main-table\n",
    "        self.main_table_current_last_elem = 0 #current(after scrolling) last scraped element of main-table\n",
    "\n",
    "        #data-storring variables\n",
    "        self.periods_table_data = [] #store data from periods table (annual or quarter)\n",
    "        self.main_table_data = [] #store data from the main-table like revenue, \n",
    "        self.all_scraped_data = pd.DataFrame()\n",
    "        \n",
    "        self.driver.get(url)\n",
    "\n",
    "        #find advertise window button and click it\n",
    "        if self.ad_clicked == False:\n",
    "            self.ad_button = self.driver.find_element_by_xpath('//button[@class=\"Button__StyledButton-a1qza5-0 cUAUIG\"]')\n",
    "            self.ad_button.click()\n",
    "            self.ad_clicked = True\n",
    "\n",
    "\n",
    "        self.horizontal_bar_width = self.driver.find_element_by_id('jqxScrollOuterWraphorizontalScrollBarjqxgrid').rect['width']\n",
    "        self.slider = self.driver.find_element_by_id('jqxScrollThumbhorizontalScrollBarjqxgrid')\n",
    "        \n",
    "        \n",
    "        #get table-parameter-names\n",
    "        full_table_params = []\n",
    "        full_table_params_and_values = []\n",
    "        full_table = self.driver.find_element_by_css_selector(\"#contentjqxgrid\")\n",
    "        full_table = full_table.text.splitlines()\n",
    "        for item in full_table:\n",
    "            if item[-1].isdigit()==False and item[-1] != \"-\" and item !=\"www.jqwidgets.com\" :\n",
    "                full_table_params.append(item)\n",
    "\n",
    "        j=-1\n",
    "        for item in full_table:\n",
    "            if item == \"www.jqwidgets.com\":\n",
    "                continue\n",
    "            elif item in full_table_params:\n",
    "                j=j+1\n",
    "                full_table_params_and_values.append([item])\n",
    "                continue\n",
    "            full_table_params_and_values[j].append(item)\n",
    "\n",
    "\n",
    "        print(full_table_params_and_values)\n",
    "        self.driver.close()\n",
    "        raise SystemExit(\"Stop right there!\")\n",
    "\n",
    "        #scroll table from left to right and scrape data\n",
    "        while (self.periods_table_prev_last_elem != self.periods_table_current_last_elem): #and (self.main_table_prev_last_elem != self.main_table_current_last_elem):\n",
    "            \n",
    "            ##########################################################################\n",
    "            # full_table = self.driver.find_element_by_xpath(\"//div[@id='jqxgrid']\")\n",
    "            # full_table = self.driver.find_element_by_css_selector(\"#contentjqxgrid\")\n",
    "            # full_table = full_table.text.splitlines()\n",
    "            # self.temp_data.append(full_table)\n",
    "            # print(full_table)\n",
    "            ###########################################################################\n",
    "\n",
    "            #set the last element from the last scrape as previous one\n",
    "            self.periods_table_prev_last_elem = self.periods_table_current_last_elem\n",
    "            self.main_table_prev_last_elem = self.main_table_current_last_elem\n",
    "\n",
    "            # self.periods_table = self.driver.find_element_by_xpath('//*[@id=\"columntablejqxgrid\"]') \n",
    "            self.periods_table = self.driver.find_element_by_xpath('//*[@id=\"contentjqxgrid\"]/div[1]') #this is the header row in the table, containing the dates.\n",
    "            # print(self.periods_table.text)\n",
    "            #parameters_table = self.driver.find_element_by_xpath('//*[@id=\"contentjqxgrid\"]/div[2]')\n",
    "            self.periods_table_split = self.periods_table.text.splitlines() #split the periods to array\n",
    "            self.periods_table_current_last_elem = self.periods_table_split[-1] #get the last element from the table\n",
    "            # print(len(self.periods_table_split), \"periods\")\n",
    "            # print('last-element={}'.format(self.periods_table_current_last_elem))\n",
    "\n",
    "            # main_table = self.driver.find_elements_by_xpath('//div[@role=\"row\"]') #get the main table\n",
    "            main_table = self.driver.find_elements_by_xpath('//*[@id=\"contentjqxgrid\"]/div[2]/div/div') #get the main table\n",
    "            self.main_table_current_last_elem = main_table[0].text.splitlines()[-1]\n",
    "            # print(len(main_table[0].text.splitlines()), \"main\")\n",
    "            #print(self.main_table_current_last_elem)\n",
    "            \n",
    "            #this is the innitial position of the table, no scrolling has been executed yet\n",
    "            if self.periods_table_prev_last_elem == 0: \n",
    "                #add scraped elements of periods table to a temp-variable \n",
    "                self.periods_table_data.extend(self.periods_table_split) \n",
    "                \n",
    "                #add the scraped elements of main-table to a temp-variable\n",
    "                for i in range (len(main_table)): #add all parameter to table\n",
    "                    main_table_split = main_table[i].text.splitlines()\n",
    "                    self.main_table_data.append(main_table_split)\n",
    "\n",
    "            else:\n",
    "                #find the previous last element when the periods-table is scrolled to the right\n",
    "                self.periods_table_prev_last_elem_index = self.periods_table_split.index(self.periods_table_prev_last_elem)\n",
    "                #add the new elements after the periods-table is scrolled to the right\n",
    "                self.periods_table_data.extend(self.periods_table_split[self.periods_table_prev_last_elem_index+1:]) \n",
    "                # print(self.periods_table_split)\n",
    "                \n",
    "                #find the previous last element when the main-table is scrolled to the right\n",
    "                self.main_table_prev_last_elem_index = main_table[0].text.splitlines().index(self.main_table_prev_last_elem) \n",
    "                # print(main_table[0].text.splitlines())\n",
    "                #add the new elements after the main-table is scrolled to the right\n",
    "                for row in range (len(main_table)): #add the new data to the table after scrolling table to the right\n",
    "                    main_table_split = main_table[row].text.splitlines()\n",
    "                    self.main_table_data[row].extend(main_table_split[self.main_table_prev_last_elem_index+1:])\n",
    "                \n",
    "\n",
    "\n",
    "                # raise SystemExit(\"Stop right there!\")\n",
    "\n",
    "            # Ensure the slider is in view\n",
    "            self.slider.location_once_scrolled_into_view\n",
    "            #scroll table to the right\n",
    "            ActionChains(self.driver).click_and_hold(self.slider).move_by_offset(self.horizontal_bar_width/5, 0).release().perform()\n",
    "\n",
    "        \n",
    "        #add the scraped data to pandas dataframe\n",
    "        for i in range (len(self.main_table_data)):\n",
    "            #create temp dataframe with scraped data\n",
    "            # print(self.main_table_data[i], self.periods_table_data)\n",
    "            data = pd.DataFrame([self.main_table_data[i][1:]], columns=self.periods_table_data[1:], index=[self.main_table_data[i][0]]) \n",
    "            \n",
    "            #append temp-dataframe row to global dataframe\n",
    "            self.all_scraped_data = pd.concat([self.all_scraped_data, data]) \n",
    "\n",
    "        \n",
    "        #return scraped data\n",
    "        return self.all_scraped_data\n",
    "    \n",
    "    def runFullTableScrape(self, url):\n",
    "\n",
    "        self.driver.get(url)\n",
    "\n",
    "        #variables\n",
    "        full_table_params = [] #collect table parameters - Periods, Revenue, Cost Of Goods Sold ..\n",
    "        full_table_params_and_values = [] #collect parameters and values\n",
    "        full_table_params_and_values_set = [] #store scraped data as array-of-sets instead of array-of-arrays\n",
    "\n",
    "        #find advertise window button and click it\n",
    "        if self.ad_clicked == False:\n",
    "            self.ad_button = self.driver.find_element_by_xpath('//button[@class=\"Button__StyledButton-a1qza5-0 cUAUIG\"]')\n",
    "            self.ad_button.click()\n",
    "            self.ad_clicked = True\n",
    "\n",
    "\n",
    "        self.horizontal_bar_width = self.driver.find_element_by_id('jqxScrollOuterWraphorizontalScrollBarjqxgrid').rect['width']\n",
    "        self.slider = self.driver.find_element_by_id('jqxScrollThumbhorizontalScrollBarjqxgrid')\n",
    "\n",
    "        while True:\n",
    "\n",
    "            #get table-parameter-names\n",
    "            full_table = self.driver.find_element_by_css_selector(\"#contentjqxgrid\")\n",
    "            full_table = full_table.text.splitlines()\n",
    "            for item in full_table:\n",
    "                if item[-1].isdigit()==False and item[-1] != \"-\" and item !=\"www.jqwidgets.com\" :\n",
    "                    full_table_params.append(item)\n",
    "\n",
    "            #get table parameters names and values\n",
    "            j=-1\n",
    "            for item in full_table:\n",
    "                if item == \"www.jqwidgets.com\":\n",
    "                    continue\n",
    "                elif item in full_table_params:\n",
    "                    j=j+1\n",
    "                    full_table_params_and_values.append([item])\n",
    "                    continue\n",
    "                full_table_params_and_values[j].append(item)\n",
    "\n",
    "            #convert data from array-of-arrays to array-of-sets\n",
    "            for i in range(len(full_table_params_and_values)):\n",
    "                temp_set = set(full_table_params_and_values[i])\n",
    "                full_table_params_and_values_set.append(item)\n",
    "\n",
    "            # print(full_table_params_and_values)\n",
    "            self.driver.close()\n",
    "            raise SystemExit(\"Stop right there!\")\n",
    "\n",
    "\n",
    "tesla = ScrapeMacrotrend(\"tesla\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5b67a964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company-url=AAPL/apple\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Revenue'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\indexes\\base.py:3800\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3799\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3800\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3801\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Revenue'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mg:\\My Drive\\Investing\\Programming\\Selenium Web Scrape 04.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 108>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/g%3A/My%20Drive/Investing/Programming/Selenium%20Web%20Scrape%2004.ipynb#W4sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m input_data\n\u001b[0;32m    <a href='vscode-notebook-cell:/g%3A/My%20Drive/Investing/Programming/Selenium%20Web%20Scrape%2004.ipynb#W4sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m company_data\u001b[39m.\u001b[39mapply(fixData)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/g%3A/My%20Drive/Investing/Programming/Selenium%20Web%20Scrape%2004.ipynb#W4sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m company_data[\u001b[39m\"\u001b[39;49m\u001b[39mRevenue\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39mplot()\n\u001b[0;32m    <a href='vscode-notebook-cell:/g%3A/My%20Drive/Investing/Programming/Selenium%20Web%20Scrape%2004.ipynb#W4sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m company_data[\u001b[39m\"\u001b[39m\u001b[39mGross Profit\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mplot()\n\u001b[0;32m    <a href='vscode-notebook-cell:/g%3A/My%20Drive/Investing/Programming/Selenium%20Web%20Scrape%2004.ipynb#W4sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m plt\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\frame.py:3805\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3803\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3804\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3805\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3806\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3807\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3800\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3801\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3804\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3806\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Revenue'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from html.parser import HTMLParser\n",
    "import pandas as pd\n",
    "\n",
    "#HTML parser that help to parse html-strings\n",
    "#https://www.educative.io/answers/what-is-the-html-parser-in-python\n",
    "class Parser(HTMLParser):\n",
    "  def handle_data(self, data):\n",
    "    self.output = data\n",
    "parser = Parser()\n",
    "\n",
    "ticker_search = 'AAPL'\n",
    "\n",
    "#get ticker data\n",
    "tickers_url = 'https://www.macrotrends.net/assets/php/ticker_search_list.php?_=1664947632720'\n",
    "data_tickers = requests.get(tickers_url)\n",
    "data_tickers = data_tickers.text\n",
    "data_tickers_json = json.loads(data_tickers)\n",
    "\n",
    "#search for the company-url\n",
    "for item in data_tickers_json:\n",
    "    # print(item['s'])\n",
    "    if ticker_search in item['s']: #example: AAPL/apple\n",
    "        company_page_url = item['s']\n",
    "        print(\"company-url={}\".format(company_page_url))\n",
    "\n",
    "# url = 'https://www.macrotrends.net/stocks/charts/TSLA/tesla/income-statement?freq=Q'\n",
    "url = 'https://www.macrotrends.net/stocks/charts/' + company_page_url + '/income-statement?freq=Q'\n",
    "\n",
    "\n",
    "page = requests.get(url)\n",
    "page_lines = page.text.splitlines()\n",
    "for line in page_lines:\n",
    "    if 'var originalData =' in line: #the line where all the data is storred\n",
    "        data = line #store the information in a variable\n",
    "\n",
    "data = data[20:-1] # remove the the unneeded informatino from the line\n",
    "data_json = json.loads(data) #convert the line to a json-object\n",
    "\n",
    "#the following vars are needed when looping through the json_\n",
    "data_index = [] #collect the data-names\n",
    "data_column = [] #collect the data-dates\n",
    "data_values = [] #collect the data-values\n",
    "\n",
    "#loop over \n",
    "for item in data_json:\n",
    "\n",
    "    #reset temp-vars for the next loop\n",
    "    data_values_temp = [] #store data-values for current loop\n",
    "    data_column_temp = [] #store data-dates for current loop\n",
    "\n",
    "    for key in item.items() :\n",
    "        # print(key[0], key[1])\n",
    "\n",
    "        if (key[0] != 'field_name') and (key[0] != 'popup_icon'): # dates and values\n",
    "            # print(key[0], key[1])\n",
    "            data_column_temp.append(key[0]) #dates\n",
    "            data_values_temp.append(key[1]) #values\n",
    "\n",
    "        elif key[0] == 'field_name': #data-names\n",
    "            # print(key[1])\n",
    "            parser.feed(key[1])\n",
    "            # print(parser.output)\n",
    "            data_index.append(parser.output) # xxx = \"<a href='/stocks/charts/TSLA/tesla/cost-goods-sold'>Cost Of Goods Sold</a>\"\n",
    "\n",
    "        elif key[0]== 'popup_icon': #data-graph link (not needed)\n",
    "            continue\n",
    "            print(key[1])\n",
    "\n",
    "    #add temp-vars to permanent ones\n",
    "    data_values.append(data_values_temp)\n",
    "    data_column.append(data_column_temp)\n",
    "\n",
    " \n",
    "\n",
    "# pd.DataFrame(data=data_values, index=data_index, columns=data_column, dtype=None, copy=None)\n",
    "company_data = pd.DataFrame(data=data_values, index=data_index, columns=data_column[0])\n",
    "\n",
    "\n",
    "# company_data.dtypes\n",
    "# company_data.columns\n",
    "# company_data.index\n",
    "# url\n",
    "# company_data.loc['Revenue']\n",
    "# company_data.loc['EPS - Earnings Per Share']\n",
    "\n",
    "# company_data.columns = pd.to_datetime(company_data.columns) #change columns data-type to datetime.\n",
    "# company_data.columns\n",
    "\n",
    "#apply-function that goes over all dataframe-elements and converts them to numeric value if possible\n",
    "def fixData(input_data):\n",
    "    \n",
    "    for i in range(len(input_data)):\n",
    "      \n",
    "        try:\n",
    "            input_data[i] = pd.to_numeric(input_data[i])\n",
    "            # print(type(input_data[i]), \"converted to numeric\")\n",
    "\n",
    "        except:\n",
    "            # pass\n",
    "            print(input_data[i], \"Can't convert to numeric\")\n",
    "        \n",
    "    return input_data\n",
    "    \n",
    "company_data.apply(fixData)\n",
    "\n",
    "company_data[\"Revenue\"].plot()\n",
    "company_data[\"Gross Profit\"].plot()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46832d9-cc4e-4c87-9c83-76bec4211338",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple.financial_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b675dc3-dadd-4bdd-be32-13e204e1c437",
   "metadata": {},
   "outputs": [],
   "source": [
    "periods_table_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7673c4-8eb5-43ad-9bd9-242e5bf34931",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677141ef-ba7c-4855-8d9f-cdd2511a836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove \"$\" from the values\n",
    "def fixData(input_data):\n",
    "    \n",
    "    for i in range(len(input_data)):\n",
    "        input_data[i]=input_data[i].replace(\"$\",\"\")\n",
    "        input_data[i]=input_data[i].replace(\",\",\".\")\n",
    "        # try:\n",
    "        #     input_data[i] =  float(input_data[i])\n",
    "        #     # print(type(input_data[i]), input_data[i])\n",
    "        # except:\n",
    "        #     pass\n",
    "        #     # print(input_data[i],\"can't convert to float\")\n",
    "        \n",
    "        try:\n",
    "            input_data[i] = pd.to_numeric(input_data[i])\n",
    "            # print(type(input_data[i]), \"converted to numeric\")\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "            # print(\"Can't convert to numeric\")\n",
    "        \n",
    "    return input_data\n",
    "    \n",
    "\n",
    "c_data = company_data.copy()\n",
    "c_data.apply(fixData)\n",
    "# c_data.apply(pd.to_numeric)\n",
    "c_data = c_data.T #transpose the table\n",
    "# c_data.index.to_datetime()\n",
    "c_data.index = pd.to_datetime(c_data.index) #change index data-type to datetime.\n",
    "c_data\n",
    "\n",
    "# c_data[\"Revenue\"] = pd.to_numeric(c_data[\"Revenue\"]) #convert data-type from string to number\n",
    "# c_data[\"Gross Profit\"] = pd.to_numeric(c_data[\"Gross Profit\"]) #convert data-type from string to number\n",
    "c_data[\"Revenue\"].plot()\n",
    "c_data[\"Gross Profit\"].plot()\n",
    "plt.show()\n",
    "\n",
    "# company_data\n",
    "#DataFrame.apply(func, axis=0, raw=False, result_type=None, args=(), **kwargs)\n",
    "# df = pd.DataFrame({\n",
    "#    'pig': [20, 18, 489, 675, 1776],\n",
    "#    'horse': [4, 25, 281, 600, 1900]\n",
    "#    }, index=[1990, 1997, 2003, 2009, 2014])\n",
    "# lines = df.plot.line()\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d95c77e-b7fb-474b-970d-f4dd1edfb9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_data.T.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b5234c3a68123d47bedd5252b78f01a988a4bd5dcba8aa660c71661565635d49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
