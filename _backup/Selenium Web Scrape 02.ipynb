{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4b8674-3a64-4411-8482-f28310839f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCRAPE MULTIPLE FINANCIAL PARAMETERS FROM SINGLE PAGE / V2 / Faster scrolling using drag&drop apprach\n",
    "#https://stackoverflow.com/questions/62119348/how-to-scroll-horizontally-using-selenium-chromedriver-in-python\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260d96d4-8724-483f-abc2-6ad3457a4103",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ToDo\n",
    "#Add Support for balanse-sheet,cashflow-statement,financial-ratios and other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252d804e-f8b5-41a3-ba8d-d034022a6303",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScrapeMacrotrend:\n",
    "    \n",
    "    def __init__(self, ticker_name):\n",
    "        \n",
    "        self.start = time.time() #start timer\n",
    "        self.ticker_name = ticker_name #ticker or company name\n",
    "        \n",
    "        #innitialize and set chrome-webdriver options\n",
    "        self.chrome_options = Options()\n",
    "        #self.chrome_options.add_argument(\"--window-size=1000,1080\")\n",
    "        #self.chrome_options.add_argument(\"--headless\")\n",
    "        \n",
    "        self.driver = webdriver.Chrome(\"G:\\My Drive\\Investing\\Programming\\chromedriver.exe\", options=self.chrome_options)\n",
    "        self.driver.implicitly_wait(5)\n",
    "        self.driver.maximize_window()\n",
    "        self.base_url = \"https://www.macrotrends.net/\"\n",
    "        \n",
    "        self.ad_clicked = False\n",
    "        self.company_url = self.getCompanyURL()\n",
    "        \n",
    "        #scrape main pages\n",
    "        self.income_statement = self.runScraper(self.company_url + \"/income-statement?freq=Q\")\n",
    "        self.balance_sheet = self.runScraper(self.company_url + \"/balance-sheet?freq=Q\")\n",
    "        self.cash_flow_statement = self.runScraper(self.company_url + \"/cash-flow-statement?freq=Q\")\n",
    "        self.financial_ratios = self.runScraper(self.company_url + \"/financial-ratios?freq=Q\")\n",
    "        \n",
    "        self.driver.close() #close chrome-webdriver\n",
    "        self.end = time.time() #stop timer\n",
    "        print('Complete time: {}s'.format(self.end - self.start))\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    def getCompanyURL(self): #this function finds the company url f.e. https://www.macrotrends.net/stocks/charts/AAPL/apple\n",
    "        self.driver.get(self.base_url)\n",
    "        self.search_box = self.driver.find_element_by_css_selector(\"input.js-typeahead\")\n",
    "        self.search_box.send_keys(self.ticker_name)\n",
    "        \n",
    "        #get company url\n",
    "        self.company_url = self.driver.find_element_by_xpath(\"//li[1]/a\") #xpath element of the company urls\n",
    "        self.company_url = self.company_url.get_attribute('href') #get the href-value\n",
    "        self.company_url = self.company_url.rsplit(\"/\", 1)[0] #split url to remove unnecesery data\n",
    "        \n",
    "        return self.company_url\n",
    "        \n",
    "        \n",
    "    def runScraper(self, url):\n",
    "        \n",
    "        #scraper variables\n",
    "        self.periods_table_prev_last_elem = -1 #previously(before scrolling) last scraped element of periods-table\n",
    "        self.periods_table_current_last_elem = 0 #current(after scrolling) last scraped element of periods-table\n",
    "        self.main_table_prev_last_elem = -1  #previously(before scrolling) last scraped element of main-table\n",
    "        self.main_table_current_last_elem = 0 #current(after scrolling) last scraped element of main-table\n",
    "\n",
    "        #data-storring variables\n",
    "        self.periods_table_data = [] #store data from periods table (annual or quarter)\n",
    "        self.main_table_data = [] #store data from the main-table like revenue, \n",
    "        self.all_scraped_data = pd.DataFrame()\n",
    "        \n",
    "        self.driver.get(url)\n",
    "\n",
    "        #find advertise window button and click it\n",
    "        if self.ad_clicked == False:\n",
    "            self.ad_button = self.driver.find_element_by_xpath('//button[@class=\"Button__StyledButton-a1qza5-0 cUAUIG\"]')\n",
    "            self.ad_button.click()\n",
    "            self.ad_clicked = True\n",
    "\n",
    "\n",
    "        self.horizontal_bar_width = self.driver.find_element_by_id('jqxScrollOuterWraphorizontalScrollBarjqxgrid').rect['width']\n",
    "        self.slider = self.driver.find_element_by_id('jqxScrollThumbhorizontalScrollBarjqxgrid')   \n",
    "\n",
    "        #scroll table from left to right and scrape data\n",
    "        while (self.periods_table_prev_last_elem != self.periods_table_current_last_elem): #and (self.main_table_prev_last_elem != self.main_table_current_last_elem):\n",
    "            \n",
    "\n",
    "            #set the last element from the last scrape as previous one\n",
    "            self.periods_table_prev_last_elem = self.periods_table_current_last_elem\n",
    "            self.main_table_prev_last_elem = self.main_table_current_last_elem\n",
    "\n",
    "            # self.periods_table = self.driver.find_element_by_xpath('//*[@id=\"columntablejqxgrid\"]') \n",
    "            self.periods_table = self.driver.find_element_by_xpath('//*[@id=\"contentjqxgrid\"]/div[1]') #this is the header row in the table, containing the dates.\n",
    "            # print(self.periods_table.text)\n",
    "            #parameters_table = self.driver.find_element_by_xpath('//*[@id=\"contentjqxgrid\"]/div[2]')\n",
    "            self.periods_table_split = self.periods_table.text.splitlines() #split the periods to array\n",
    "            self.periods_table_current_last_elem = self.periods_table_split[-1] #get the last element from the table\n",
    "            # print(len(self.periods_table_split), \"periods\")\n",
    "            # print('last-element={}'.format(self.periods_table_current_last_elem))\n",
    "\n",
    "            # main_table = self.driver.find_elements_by_xpath('//div[@role=\"row\"]') #get the main table\n",
    "            main_table = self.driver.find_elements_by_xpath('//*[@id=\"contentjqxgrid\"]/div[2]/div/div') #get the main table\n",
    "            self.main_table_current_last_elem = main_table[0].text.splitlines()[-1]\n",
    "            # print(len(main_table[0].text.splitlines()), \"main\")\n",
    "            #print(self.main_table_current_last_elem)\n",
    "            \n",
    "            #this is the innitial position of the table, no scrolling has been executed yet\n",
    "            if self.periods_table_prev_last_elem == 0: \n",
    "                #add scraped elements of periods table to a temp-variable \n",
    "                self.periods_table_data.extend(self.periods_table_split) \n",
    "                \n",
    "                #add the scraped elements of main-table to a temp-variable\n",
    "                for i in range (len(main_table)): #add all parameter to table\n",
    "                    main_table_split = main_table[i].text.splitlines()\n",
    "                    self.main_table_data.append(main_table_split)\n",
    "\n",
    "            else:\n",
    "                #find the previous last element when the periods-table is scrolled to the right\n",
    "                self.periods_table_prev_last_elem_index = self.periods_table_split.index(self.periods_table_prev_last_elem)\n",
    "                #add the new elements after the periods-table is scrolled to the right\n",
    "                self.periods_table_data.extend(self.periods_table_split[self.periods_table_prev_last_elem_index+1:]) \n",
    "                # print(self.periods_table_split)\n",
    "                \n",
    "                #find the previous last element when the main-table is scrolled to the right\n",
    "                self.main_table_prev_last_elem_index = main_table[0].text.splitlines().index(self.main_table_prev_last_elem) \n",
    "                # print(main_table[0].text.splitlines())\n",
    "                #add the new elements after the main-table is scrolled to the right\n",
    "                for row in range (len(main_table)): #add the new data to the table after scrolling table to the right\n",
    "                    main_table_split = main_table[row].text.splitlines()\n",
    "                    self.main_table_data[row].extend(main_table_split[self.main_table_prev_last_elem_index+1:])\n",
    "                \n",
    "\n",
    "\n",
    "                # raise SystemExit(\"Stop right there!\")\n",
    "\n",
    "            # Ensure the slider is in view\n",
    "            self.slider.location_once_scrolled_into_view\n",
    "            #scroll table to the right\n",
    "            ActionChains(self.driver).click_and_hold(self.slider).move_by_offset(self.horizontal_bar_width/5, 0).release().perform()\n",
    "\n",
    "        \n",
    "        #add the scraped data to pandas dataframe\n",
    "        for i in range (len(self.main_table_data)):\n",
    "            #create temp dataframe with scraped data\n",
    "            # print(self.main_table_data[i], self.periods_table_data)\n",
    "            data = pd.DataFrame([self.main_table_data[i][1:]], columns=self.periods_table_data[1:], index=[self.main_table_data[i][0]]) \n",
    "            \n",
    "            #append temp-dataframe row to global dataframe\n",
    "            self.all_scraped_data = pd.concat([self.all_scraped_data, data]) \n",
    "\n",
    "        \n",
    "        #return scraped data\n",
    "        return self.all_scraped_data\n",
    "        \n",
    "# apple = ScrapeMacrotrend(\"https://www.macrotrends.net/stocks/charts/AAPL/apple/income-statement?freq=Q\")\n",
    "# apple = ScrapeMacrotrend(\"aapl\")\n",
    "apple = ScrapeMacrotrend(\"apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46832d9-cc4e-4c87-9c83-76bec4211338",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple.cash_flow_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7673c4-8eb5-43ad-9bd9-242e5bf34931",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677141ef-ba7c-4855-8d9f-cdd2511a836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove \"$\" from the values\n",
    "def fixData(input_data):\n",
    "    \n",
    "    for i in range(len(input_data)):\n",
    "        input_data[i]=input_data[i].replace(\"$\",\"\")\n",
    "        input_data[i]=input_data[i].replace(\",\",\".\")\n",
    "        # try:\n",
    "        #     input_data[i] =  float(input_data[i])\n",
    "        #     # print(type(input_data[i]), input_data[i])\n",
    "        # except:\n",
    "        #     pass\n",
    "        #     # print(input_data[i],\"can't convert to float\")\n",
    "        \n",
    "        try:\n",
    "            input_data[i] = pd.to_numeric(input_data[i])\n",
    "            # print(type(input_data[i]), \"converted to numeric\")\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "            # print(\"Can't convert to numeric\")\n",
    "        \n",
    "    return input_data\n",
    "    \n",
    "\n",
    "c_data = company_data.copy()\n",
    "c_data.apply(fixData)\n",
    "# c_data.apply(pd.to_numeric)\n",
    "c_data = c_data.T #transpose the table\n",
    "# c_data.index.to_datetime()\n",
    "c_data.index = pd.to_datetime(c_data.index) #change index data-type to datetime.\n",
    "c_data\n",
    "\n",
    "# c_data[\"Revenue\"] = pd.to_numeric(c_data[\"Revenue\"]) #convert data-type from string to number\n",
    "# c_data[\"Gross Profit\"] = pd.to_numeric(c_data[\"Gross Profit\"]) #convert data-type from string to number\n",
    "c_data[\"Revenue\"].plot()\n",
    "c_data[\"Gross Profit\"].plot()\n",
    "plt.show()\n",
    "\n",
    "# company_data\n",
    "#DataFrame.apply(func, axis=0, raw=False, result_type=None, args=(), **kwargs)\n",
    "# df = pd.DataFrame({\n",
    "#    'pig': [20, 18, 489, 675, 1776],\n",
    "#    'horse': [4, 25, 281, 600, 1900]\n",
    "#    }, index=[1990, 1997, 2003, 2009, 2014])\n",
    "# lines = df.plot.line()\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d95c77e-b7fb-474b-970d-f4dd1edfb9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_data.T.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2867b093",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b5234c3a68123d47bedd5252b78f01a988a4bd5dcba8aa660c71661565635d49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
